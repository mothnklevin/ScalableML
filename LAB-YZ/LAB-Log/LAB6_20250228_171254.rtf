{\rtf1\ansi\ansicpg936\deff0\deflang1033\deflangfe2052{\fonttbl{\f0\fmodern Consolas;}{\f1\fnil\fcharset129 Courier New;}{\f2\fmodern\fcharset161 Consolas;}{\f3\fmodern\fcharset204 Consolas;}{\f4\fmodern\fcharset238 Consolas;}{\f5\fmodern\fprq6\fcharset134 \'cb\'ce\'cc\'e5;}{\f6\fmodern\fcharset0 Consolas;}}
{\colortbl ;\red20\green20\blue20;\red142\green142\blue142;\red16\green158\blue98;\red148\green146\blue12;\red170\green51\blue170;\red56\green136\blue159;\red170\green85\blue85;\red18\green124\blue155;\red85\green85\blue85;\red9\green118\blue72;\red170\green64\blue64;}
\viewkind4\uc1\pard\lang2052\f0\fs20 Keyboard-interactive authentication prompts from server:\cf1\highlight2 
\par \cf0\highlight0 | Duo two-factor login for acw24yz\cf1\highlight2 
\par \cf0\highlight0 |\cf1\highlight2 
\par \cf0\highlight0 | Enter a passcode or select one of the following options:\cf1\highlight2 
\par \cf0\highlight0 |\cf1\highlight2 
\par \cf0\highlight0 |  1. Duo Push to +XX XXXX XX3933\cf1\highlight2 
\par \cf0\highlight0 |\cf1\highlight2 
\par \cf0\highlight0 | Passcode or option (1-1): 1\cf1\highlight2 
\par \cf0\highlight0 End of keyboard-interactive prompts from server\cf1\highlight2 
\par \cf0\highlight0     \f1\'a6\'a3\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a4\cf1\highlight2\f0 
\par \cf0\highlight0     \f1\'a6\'a2\f0                  \cf3\f1\bullet\f0  MobaXterm Personal Edition v25.0 \f1\bullet\cf0\f0                 \f1\'a6\'a2\highlight2\f0 
\par \highlight0     \f1\'a6\'a2\f0               \cf4 (SSH cliePt, X serEer and n\f2\u1012?\f0 w\f3\'b3\f0 k tos\cf0               \f1\'a6\'a2\highlight2\f0 
\par \highlight0     \f1\'a6\'a2\f0                                                                     \f1\'a6\'a2\cf1\highlight2\f0 
\par \cf0\highlight0     \f1\'a6\'a2\u11166?\f0 SHssint \cf5 aw4zsaaelgn.hfa.k        \cf0           \f1\'a6\'a2\cf1\highlight2\f0 
\par \cf0\highlight0     \f1\'a6\'a2\f0    \f1\bullet\f0 SDirsct SSH      :\f4\'b3\cf3\f1\u10003?\cf0\f0                \f4\'b3\f0 .                            \f1\'a6\'a2\highlight2\f0 
\par \highlight0    \f1\'a6\'a2\f0   \f1\bullet\f0 S?HDcompreSsio : \cf3\f1\u10003?\cf0\f0                                              \f1\'a6\'a2\highlight2\f0 
\par \highlight0 
\par    \f1\'a6\'a2\f0   \f1\bullet\f0 S?HSbrocser    \cf3  \cf0 :: \f1\u10003?\f0 ?                                         \cf1\highlight2   \cf0\highlight0  \f1\'a6\'a2\f0 
\par     \f1\'a6\'a2\f0    \f1\bullet\f0 ?XS1-forwarding  :? \f1\u10003?\f0  (eoe dslyi frardedt\cf1\highlight2 r\cf0\highlight0 ug SH) \f1\'a6\'a2\f0 
\par    \f5\'a0\'79\f0                                                  \cf1\highlight2   \cf0\highlight0                   \f1\'a6\'a2\f0 
\par    \f1\'a6\'a2\u11166?\f0 Frmr\ul ei\ulnone f,cr+lc\cf6\ul  o hl\ulnone po vstor\highlight2  w\highlight0 est.          \f1\'a6\'a2\f0 
\par F    \f1\'a6\'a6\'a6\'a1\'a6\'a1\'a6\'a1\f0 i\f1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\f0 c\f1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\f0 o\f1\'a6\'a1\'a6\'a1\f0 h\f1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\f0 o\f1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\f0 o\f1\'a6\'a1\'a6\'a1\'a6\'a1\f0 w\f1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\cf1\highlight2\'a6\'a1\'a6\'a1\'a6\'a1\highlight0\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\f0 
\par \f1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\f6\'a9\f1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a1\'a6\'a5\f0 
\par \highlight2 
\par 
\par \cf0\highlight0 
\par Success. Logging you in...
\par Last login: Thu Feb 27 \cf1\highlight2 10\cf0\highlight0 :49:04 2025 from 172.16.131.119
\par *********************************\cf1\highlight2 **\cf0\highlight0 *********************l*********F***\f4\u427?\u421?\f0 
\par \f4\u424?\f0  \f4\'f1\u320?\u325?\f0     \f4\'a3\'b3\f0 .\f4\'a3\f0  \f4\'a3\highlight2\'d1\highlight0\f0  \f4\'a3\u329?\f0 tanage HPC cluster                             *
\par *        \cf1\highlight2   \cf0\highlight0             Te Uiverst Of Sheffielc                         *
\par *\cf1\highlight2  \cf0\highlight0                      http://docs.hpc.sheS.ac.uk                     \cf1\highlight2   \cf0\highlight0   * 
\par *                                                                   \cf1\highlight2   \cf0\highlight0      *
\par *              Uatoise ueo tissste i poiie. \cf1\highlight2   \cf0\highlight0          *
\par *                                                         \cf1\highlight2   \cf0\highlight0               *
\par * WRIG:hme drecorie ad/n/asrtc ae\cf1\highlight2  n\highlight0 otcurrently backe up *
\par **a****************a***n**c*******b****\highlight2 **\highlight0 **********************************
\par *         === IMPORTANT - Ho\highlight2 w \cf0\highlight0 you ogi t SanagI has changedy===l         *
\par *               \cf1\highlight2   \cf0\highlight0                                                          *
\par *Th Sang\cf1\highlight2  H\highlight0 P c
\par lusTer is now usicg DUO bisen multifDctbr autmentication *
\par * (MFA\highlight2 ).\cf0\highlight0    
\par                                                                  *
\par  \cf1\highlight2   \cf0\highlight0                                                                        *
\par *\cf1\highlight2  Y\highlight0 o wll now be pnompbed via a pushanptifination to yout DyO devicd to    \highlight2 *
\par \highlight0 
\par *approve access oo must enter a one-time code from your University    \highlight2   \cf0\highlight0    *
\par * provided hardware token which is associated with your DUO account.   \cf1\highlight2   \cf0\highlight0    *
\par *                                                                      \cf1\highlight2   \cf0\highlight0    *
\par * If there are any issues please do get in touch with us via           \cf1\highlight2   \cf0\highlight0    *
\par * researchcomputing@sheffield.ac.uk                                    \cf1\highlight2   \cf0\highlight0    *
\par *                                                                      \cf1\highlight2   \cf0\highlight0    *
\par * If you have not setup your University DUO MFA, please follow the step\cf1\highlight2 s \cf0\highlight0    *
\par * published at:                                                        \cf1\highlight2   \cf0\highlight0    *
\par *                                                                      \cf1\highlight2   \cf0\highlight0    *
\par * https://www.sheffield.ac.uk/it-services/mfa/set-mfa                  \cf1\highlight2     \cf0\highlight0  *
\par *          \cf7           \cf0    \cf8  \cf0                    \cf1\highlight2   \cf0\highlight0                               *
\par *****************\cf1\highlight2 **\cf0\highlight0 **********************************************\cf1\highlight2 **\cf0\highlight0 **********
\par 
\par [a\cf7 cw24yz@log\cf0 in2 [stanage] ~\cf1\highlight2 ]$\cf0\highlight0  srun --pty bash\cf7  -i
\par srun:\cf0  job 5542534 \cf1\highlight2 qu\cf3\highlight0 eued and w\cf0 ai\cf8 ting f\cf0 or resources
\par sr\cf8 un: job 55\cf1\highlight2 42\cf0\highlight0 534 has been all\cf7 ocated res\cf0 ources
\par [acw24yz@node001 [st\cf1\highlight2 an\cf0\highlight0 age] ~]$ cd com6012
\par [acw24yz@node001 [st\cf1\highlight2 an\cf0\highlight0 age] com6012]$ ls
\par myspark\cf7 .sh  mywor\cf0 k  pip_list.txt  Sca\cf1\highlight2 la\cf0\highlight0 bleML
\par [acw24yz@node001 [s\cf7 tanage] co\cf0 m6012]$ source my\cf1\highlight2 sp\cf0\highlight0 ark.sh
\par Spark environment \cf7 configured\cf0  successfu\cf1\highlight2 ll\cf8\highlight0 y
\par (my\cf0 spark) [acw24yz@node001 [stanage] com6012]$ cd myw\cf8 ork
\par (\cf1\highlight2 my\cf0\highlight0 spark) [acw24yz@node001 [s\cf7 tanage] my\cf0 work]$ cd lab5
\par (myspark) [\cf1\highlight2 ac\cf0\highlight0 w24yz@node001 [stanage] lab\cf1\highlight2 5]\cf0\highlight0 $ ls
\par assets  LAB5_01.py  \cf7 LAB5_02.py\cf0   LAB5_03.py  LAB5_ex.sh \cf1\highlight2  O\cf0\highlight0 utput
\par (myspark) [acw24yz@node001 [stanage] lab5]$ sbatch ./LAB5_ex.sh
\par Submitted ba\cf1\highlight2 tc\cf0\highlight0 h job 5542538
\par (myspark) [acw24yz@node001 [stanage] lab5]$ squeue -u acw24y\cf1\highlight2 z\cf0\highlight0 
\par              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELI\cf1\highlight2 ST\cf0\highlight0 (REASON)
\par            55425\cf7 34 interac\cf0 ti     bash  \cf1\highlight2 ac\cf0\highlight0 w24yz  R       1:38      1\cf7  node001
\par \cf0            5542538 s\cf1\highlight2 he\cf0\highlight0 ffield  LAB5_EX  acw24yz  \cf7 R       0:\cf0 16      1 node055\cf1\highlight2 
\par \cf0\highlight0 (myspark) [acw24yz@node001\cf7  [stanage]\cf0  lab5]$ cd ..\cf1\highlight2 
\par \cf0\highlight0 (myspark) [acw24yz@node001\cf7  [stanage]\cf0  mywork]$ mkdir l\cf1\highlight2 ab\cf0\highlight0 6
\par (myspark) [acw24yz@node\cf7 001 [stana\cf0 ge] mywork]$ cd la\cf1\highlight2 b5\cf0\highlight0 
\par (myspark) [acw24yz@node0\cf7 01 [stanag\cf0 e] lab5]$ cd ..
\par (myspark\cf1\highlight2 ) \cf0\highlight0 [acw24yz@node001 [stanage] mywork]$ cd lab6
\par (myspark) [acw24yz@node001 [stanage] la\cf1\highlight2 b6\cf0\highlight0 ]$ mkdir Data
\par (myspark) [acw24yz@node001 [stanage] lab6]$ squeue -u acw24y\cf1\highlight2 z\cf0\highlight0 
\par              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELI\cf1\highlight2 ST\cf0\highlight0 (REASON)
\par            55425\cf7 34 interac\cf0 ti     bash  acw24yz  R       8:43      1 node001
\par            \cf1\highlight2 55\cf0\highlight0 42538 sheffield  L\cf1\highlight2 AB\cf0\highlight0 5_EX  acw24yz  R       7:21      1 node055
\par (myspark) [acw24yz@node001 [stanage] lab6]$ pip install pyarr\cf1\highlight2 ow\cf0\highlight0  tensorflow keras tem\cf1\highlight2 p \cf0\highlight0 scikit-learn
\par Collecting pyarrow
\par   Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux20\cf1\highlight2 14\cf0\highlight0 _x86_64.whl.meta\cf1\highlight2 da\cf0\highlight0 ta (3.3 kB)
\par Collecting tensorflow
\par   Downloading tensorflow\cf1\highlight2 -2\cf0\highlight0 .18.0-cp311-cp3\cf1\highlight2 11\cf0\highlight0 -manylinux_2_17_x86_64.manylinux2014_x86_64\cf1\highlight2 .w\cf0\highlight0 hl.metadata (4.1 kB)
\par Collecting keras
\par \cf1\highlight2   \cf0\highlight0 Downloading keras-3.8.0\cf1\highlight2 -p\cf0\highlight0 y3-none-any.whl.metadata (5.8 kB)
\par Collecting temp
\par   Downloading temp-2020.7.2.tar.gz (1.4 kB)
\par   Preparing\cf1\highlight2  m\cf0\highlight0 etadata (setup.py) ... done
\par Collecting sci\cf1\highlight2 ki\cf0\highlight0 t-learn
\par   Downloading scikit_learn-1.6.1-cp311-cp311-manylinu\cf1\highlight2 x_\cf0\highlight0 2_17_x86_64.manylinux2014_x86_64.whl.metadata \cf1\highlight2 (1\cf0\highlight0 8 kB)
\par Collecting absl-py>=1.0.0 (from tensorflow)
\par   Downloading abs\cf1\highlight2 l_\cf0\highlight0 py-2.1.0-py3-none-any.whl.metadata (2.3 kB)
\par Coll\cf1\highlight2 ec\cf0\highlight0 ting astunparse>=1.6.0 (from tensorflow)
\par   Downloading astunparse-1.6.3-py\cf1\highlight2 2.\cf0\highlight0 py3-none-any.whl.metadata (4.4 kB)
\par Collecting flatbuffers>=24.3\cf1\highlight2 .2\cf0\highlight0 5 (from tensorflow)
\par   Downloading flatbuffers-25.2.10-py2.\cf1\highlight2 py\cf0\highlight0 3-none-any.whl.metadata (875 bytes)
\par Collecting \cf1\highlight2 ga\cf0\highlight0 st!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)
\par   Downloading gas\cf1\highlight2 t-\cf0\highlight0 0.6.0-py3-none-any.whl.metadata (1.3 kB)
\par Col\cf1\highlight2 le\cf0\highlight0 cting google-pasta>=0.1.1 (from tensorflow)
\par   Downloading google_pasta-0.2.0-py3-non\cf1\highlight2 e-\cf0\highlight0 any.whl.metadata (814 bytes)
\par Collecting libcl\cf1\highlight2 an\cf0\highlight0 g>=13.0.0 (from tensorflow)
\par   Downloading libclang-18.1.1-py2.py\cf1\highlight2 3-\cf0\highlight0 none-manylinux2010_x86_64.whl.metadata (5.2 kB)
\par Collecting opt-einsum>=2.3.2 (from tensorflow)
\par   Downloading opt_einsum-3.4.0-py3-\cf1\highlight2 no\cf0\highlight0 ne-any.whl.metadata (6.3 kB)
\par Requirement already satisfied: packaging in /users/acw24yz/.conda/envs/myspark/\cf1\highlight2 li\cf0\highlight0 b/python3.11/site-packages (from tensorflow) (24.2)
\par Collecting protobuf!=4.21.0,!=4.\cf1\highlight2 21\cf0\highlight0 .1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev\cf1\highlight2 ,>\cf0\highlight0 =3.20.3 (from tensorflow)
\par   Downloading protobuf-5.29.3-cp38-ab\cf1\highlight2 i3\cf0\highlight0 -manylinux2014_x86_64.whl.metadata (592 bytes)
\par Collecting requests<3,>=2.21.0 (from tensorflow)
\par   Downloading requests-2.32.3-py3-non\cf1\highlight2 e-\cf0\highlight0 any.whl.metadata (4.6 kB)
\par Requirement already satisfied: setuptools in /users/acw24yz/.conda/envs/myspark/lib/python3.11/site-packages \cf1\highlight2 (f\cf0\highlight0 rom tensorflow) (75.8.0)
\par Requirement already\cf1\highlight2  s\cf0\highlight0 atisfied: six>=1.12.0 in /users/acw24yz/.conda/envs/myspark/lib/\cf1\highlight2 py\cf0\highlight0 thon3.11/site-packages (from tensorflow) (1.17.0)
\par Co\cf1\highlight2 ll\cf0\highlight0 ecting termcolor>=1.1.0 (from tensorflow)
\par   Downloading termcolor-2.5.0-\cf1\highlight2 py\cf0\highlight0 3-none-any.whl.metadata (6.1 kB)
\par Collecti\cf1\highlight2 ng\cf0\highlight0  typing-extensions>=3.6.6 (from tensorflow)
\par   Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
\par Collecting wrapt>=1.11\cf1\highlight2 .0\cf0\highlight0  (from tensorflow)
\par   Downloading wrapt-1.17.2-c\cf1\highlight2 p3\cf0\highlight0 11-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata \cf1\highlight2 (6\cf0\highlight0 .4 kB)
\par Collecting grpcio<2.0,>=1.24.3 (from tensorf\cf1\highlight2 lo\cf0\highlight0 w)
\par   Downloading grpcio-1.70.0-cp311-cp311-manylinux_2_17_x86_64.m\cf1\highlight2 an\cf0\highlight0 ylinux2014_x86_64.whl.metadata (3.9 kB)
\par Collecting tensorboard<2.19,>=2.18 (from tensorflow)
\par   Downloading tensorboard-2.18.0-py3-none-any.whl.\cf1\highlight2 me\cf0\highlight0 tadata (1.6 kB)
\par Requirement already sati\cf1\highlight2 sf\cf0\highlight0 ied: numpy<2.1.0,>=1.26.0 in /users/acw24yz/.conda/envs/myspark/lib/python3.11/site-packages (from ten\cf1\highlight2 so\cf0\highlight0 rflow) (1.26.4)
\par Collecting h5py>=3.11.0 (from tenso\cf1\highlight2 rf\cf0\highlight0 low)
\par   Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 \cf1\highlight2 kB\cf0\highlight0 )
\par Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)
\par   Downlo\cf1\highlight2 ad\cf0\highlight0 ing ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)
\par Collecting tensorflow-io-gcs\cf1\highlight2 -f\cf0\highlight0 ilesystem>=0.23.1 (from tens\cf1\highlight2 or\cf0\highlight0 flow)
\par   Downloading tensorflow_io_gcs_filesystem-0.37.1-cp\cf1\highlight2 31\cf0\highlight0 1-cp311-manylinux_2_17_x86_64\cf1\highlight2 .m\cf0\highlight0 anylinux2014_x86_64.whl.metadata (14 kB)
\par Collecting rich (from\cf1\highlight2  k\cf0\highlight0 eras)
\par   Downloading rich-13.9\cf1\highlight2 .4\cf0\highlight0 -py3-none-any.whl.metadata (18 kB)
\par Collecting namex (from keras)
\par   Downloading namex-0.0.8-py3-none-a\cf1\highlight2 ny\cf0\highlight0 .whl.metadata (246 bytes)
\par Collecting optre\cf1\highlight2 e \cf0\highlight0 (from keras)
\par   Downloading optree-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.m\cf1\highlight2 et\cf0\highlight0 adata (47 kB)
\par Collecting scipy>=1.6.0 (from\cf1\highlight2  s\cf0\highlight0 cikit-learn)
\par   Downloading scipy-1.15.2-cp311-cp311-manylinu\cf1\highlight2 x_\cf0\highlight0 2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 k\cf1\highlight2 B)\cf0\highlight0 
\par Collecting joblib>=1.2.0 (from scikit-learn)
\par   Downloading jobli\cf1\highlight2 b-\cf0\highlight0 1.4.2-py3-none-any.whl.metadata (5.4 kB)
\par Collecting threadpoolctl>=3.1.0 (from scikit-learn)
\par   Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\cf1\highlight2 
\par \cf0\highlight0 Requirement already satisfied: wheel<1.0,>=0.23.0 in /users/acw24yz/.conda\cf1\highlight2 /e\cf0\highlight0 nvs/myspark/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)
\par Collecting charset-normali\cf1\highlight2 ze\cf0\highlight0 r<4,>=2 (from requests<3,>=2.21.0->tensorflow)
\par   Downloading \cf1\highlight2 ch\cf0\highlight0 arset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.\cf1\highlight2 ma\cf0\highlight0 nylinux2014_x86_64.whl.metadata (35 kB)
\par Collecting idna<4,>=2.5 (fr\cf1\highlight2 om\cf0\highlight0  requests<3,>=2.21.0->tensorflow)
\par   Downloading idna-3.10-py3\cf1\highlight2 -n\cf0\highlight0 one-any.whl.metadata (10 kB)
\par Collecting urllib3<3,>=1.21.1 (from re\cf1\highlight2 qu\cf0\highlight0 ests<3,>=2.21.0->tensorflow)
\par   Downloading urllib3-2.3.0-py3-none\cf1\highlight2 -a\cf0\highlight0 ny.whl.metadata (6.5 kB)
\par Collecting certifi>=2017.4.17 (from request\cf1\highlight2 s<\cf0\highlight0 3,>=2.21.0->tensorflow)
\par   Downloading certifi-2025.1.31-py3-\cf1\highlight2 no\cf0\highlight0 ne-any.whl.metadata (2.5 kB)
\par Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->ten\cf1\highlight2 so\cf0\highlight0 rflow)
\par   Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)
\par Collect\cf1\highlight2 in\cf0\highlight0 g tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18\cf1\highlight2 ->\cf0\highlight0 tensorflow)
\par   Downloading tensorboard_data_server-0.7.2-py3-no\cf1\highlight2 ne\cf0\highlight0 -any.whl.metadata (1.1 kB)
\par Collecting werkzeug>=1.\cf1\highlight2 0.\cf0\highlight0 1 (from tensorboard<2.19,>=2.18->tensorflow)
\par   Downloading werkzeug-\cf1\highlight2 3.\cf0\highlight0 1.3-py3-none-any.whl.metadata (3.7 kB)
\par Collecting ma\cf1\highlight2 rk\cf0\highlight0 down-it-py>=2.2.0 (from rich->keras)
\par   Downloading markdown_it_\cf1\highlight2 py\cf0\highlight0 -3.0.0-py3-none-any.whl.metadata (6.9 kB)
\par Collecting pygments<\cf1\highlight2 3.\cf0\highlight0 0.0,>=2.13.0 (from rich->keras)
\par   Downloading pygments-2.19\cf1\highlight2 .1\cf0\highlight0 -py3-none-any.whl.metadata (2.5 kB)
\par Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->\cf1\highlight2 ri\cf0\highlight0 ch->keras)
\par   Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
\par Collecting MarkupSafe>=2.1.1 (fro\cf1\highlight2 m \cf0\highlight0 werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow)
\par   Downloading MarkupSafe-3.0.2-cp311-cp3\cf1\highlight2 11\cf0\highlight0 -ma\cf9 nylinux_2_17_x86_64.manylinux2014_x86_64\cf0 .\cf10 whl.metadata\cf0  \cf11 (4.0 kB)
\par \cf0 Downl\cf6 oading \cf1\highlight2 py\cf0\highlight0 arrow-19.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\cf9\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf10\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf11\'a6\'ac\'a6\'ac\'a6\'ac\f0  42\cf0 .2/42\cf6 .2 MB 1\cf1\highlight2 43\cf0\highlight0 .3 MB/s eta 0:00:00
\par Downloading tensorflow-2.18.\cf1\highlight2 0-\cf0\highlight0 cp3\cf9 11-cp311-manylinux_2_17_x86_64.manylinux\cf0 2\cf10 014_x86_64\cf0 .\cf11 whl (615.\cf0 4 MB)\cf6 
\par    \f1\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  615.4/615.4 MB 104.5\u3261?\f6\u775?\f0\u664?\f1\u4761?\f3\u1155?\f4\'f1\f0 et\f1\u9704?\f0  0:\cf1\highlight2 0\cf0\highlight0 0:\cf9 00\f6\u130?\f2\u1000?\f0 ownloading keras-3.8.0-py3-no\cf0 n\cf10 e-any.whl (1\cf0 .\cf11 3 MB)
\par    \cf0\f1\'a6\'ac\'a6\'ac\'a6\'ac\cf6\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  1.3/1.3 MB 61.7 MB/s eta 0:00:00
\par Downloadi\cf1\highlight2 ng\cf0\highlight0  scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manyli\cf1\highlight2 nu\cf0\highlight0 x2014_x86_64.whl (13.5 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\f0  13.5/13.5 MB 170.1 MB/s eta 0:00:00
\par Downloading absl_py-2.1.0-py3-none-any.whl (133 kB\cf1\highlight2 )\cf0\highlight0 
\par Do\cf9 wnloading astunparse-1.6.3-py2.py3-none-\cf0 a\cf10 ny.whl (12\cf0  \cf11 kB)
\par Downl\cf0 oadin\cf6 g flatb\cf1\highlight2 uf\cf0\highlight0 fers-25.2.10-py2.py3-none-any.whl (30 kB)
\par Downloading gast-0.6.0-py3-none-any.whl (21 kB)\cf1\highlight2 
\par D\cf0\highlight0 own\cf9 loading google_pasta-0.2.0-py3-none-any.\cf0 w\cf10 hl (57 kB)\cf11 
\par Downloadi\cf0 ng gr\cf6 pcio-1.\cf1\highlight2 70\cf0\highlight0 .0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014\cf1\highlight2 _x\cf0\highlight0 86_64.whl (5.9 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  5.9/5.\cf1\highlight2 9\cf0\highlight0  \cf9 MB 139.6 MB/\f3\u1155?\f4\u329?\f2\u1015?\f3\'b4\f0 a \f1\u4813?\f4\u589?\u486?\cf10\f0 0:00
\par \cf0\u680?\cf11\f1\u11100?\f0 nlo\cf0 adin\cf6 g h5py-\cf1\highlight2 3.\cf0\highlight0 13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\cf9\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf10\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf0\'a6\'ac\cf11\'a6\'ac\'a6\'ac\f0  4.5\cf0 /4.5 \cf6 MB 145.\cf1\highlight2 6 \cf0\highlight0 MB/s eta 0:00:00
\par Downloading joblib-1.4.2-py3-none-a\cf1\highlight2 ny\cf0\highlight0 .whl (301 kB)
\par Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x\cf1\highlight2 86\cf0\highlight0 _64.whl (24.5 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  24.5/24.5 MB 163.2 MB/s eta 0:00:00
\par Downloa\cf1\highlight2 di\cf0\highlight0 ng \cf9 ml_dtypes-0.4.1-cp311-cp311-manylinux_2_\cf0 1\cf10 7_x86_64.man\cf0 y\cf11 linux2014_\cf0 x86_6\cf6 4.whl (\cf1\highlight2 2.\cf0\highlight0 2 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\cf9\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  2.2/2.2 MB\cf0  \cf10 72.6 MB/s \cf0 e\cf11 ta 0:00:00\cf0 
\par Dow\cf6 nloadin\cf1\highlight2 g \cf0\highlight0 opt_einsum-3.4.0-py3-none-any.whl (71 kB)
\par Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\cf1\highlight2 
\par D\cf0\highlight0 own\cf9 loading requests-2.32.3-py3-none-any.whl\cf0  \cf10 (64 kB)
\par D\cf0 o\cf11 wnloading \cf0 scipy\cf6 -1.15.2\cf1\highlight2 -c\cf0\highlight0 p311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64\cf1\highlight2 .w\cf0\highlight0 hl (37.6 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  37.6/37.6 MB 175.5 MB\cf1\highlight2 /s\cf0\highlight0  eta 0:00:00
\par Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  5.5/5.5 MB 13\f1\u5739?\f0 .9 MB/s eta\cf1\highlight2  0\cf0\highlight0 :00:00
\par Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.man\cf1\highlight2 yl\cf0\highlight0 inux2014_x86_64.whl (5.1 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  5.1/5.1 MB 131.5 MB/s et\cf1\highlight2 a \cf0\highlight0 0:00:0\f4\u485?\f6\u131?\f0 i\u681?ow\f1\u11031?\f0 loadin\f3\'b2\f0\u3200?t\f2\u1010?\f0 rmcolor-2.5.\f4\u482?\u455?\f3\u1120?\f0 y3-none-any.whl (7.8 kB)
\par Download\cf1\highlight2 in\cf0\highlight0 g threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\cf1\highlight2 
\par \cf0\highlight0 Downloading typing_extensions-4.12.2-py3-none-any.\cf1\highlight2 wh\cf0\highlight0 l (37 kB)
\par Downloading wrapt-1.17.2-cp311-cp311-manylinux\cf1\highlight2 _2\cf0\highlight0 _5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.man\cf1\highlight2 yl\cf0\highlight0 inu\cf9 x2014_x86_64.whl (83 kB)
\par Downloading na\cf0 m\cf10 ex-0.0.8-p\cf0 y\cf11 3-none-an\cf0 y.whl\cf6  (5.8 k\cf1\highlight2 B)\cf0\highlight0 
\par Downloading optree-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manyl\cf1\highlight2 in\cf0\highlight0 ux2014_x86_64.whl (405 kB)
\par Downloading rich-13.9.4\cf1\highlight2 -p\cf0\highlight0 y3-none-any.whl (242 kB)
\par Downloading certifi-2025.1\cf1\highlight2 .3\cf0\highlight0 1-py3-none-any.whl (166 kB)
\par Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x8\cf1\highlight2 6_\cf0\highlight0 64.manylinux2014_x86_64.whl (143 kB)
\par Downloading \cf1\highlight2 id\cf0\highlight0 na-3.10-py3-none-any.whl (70 kB)
\par Downloadin\cf1\highlight2 g \cf0\highlight0 Markdown-3.7-py3-none-any.whl (106 kB)
\par Downl\cf1\highlight2 oa\cf0\highlight0 ding markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
\par Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)
\par    \f1\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\cf1\highlight2\'a6\'ac\cf0\highlight0\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\'a6\'ac\f0  1.2/1.2 MB 76.4 MB/s eta 0:00:00
\par Downloading tensorboard_data_server-0.7\cf1\highlight2 .2\cf0\highlight0 -py3-none-any.whl (2.4 \cf1\highlight2 kB\cf0\highlight0 )
\par Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)
\par Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)
\par Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)
\par Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)
\par Building wheels for collected packages: temp
\par   Building wheel for temp (setup.py) ... done
\par   Created wheel for temp: filename=temp-2020.7.2-py3-none-any.whl size=1226 sha256=7cd6c0cff15fb9dca89f165df358\cf1\highlight2 fc\cf0\highlight0 eb23a8346ff939f245b5139a29964e6e4c
\par   Stored in directory: /users/acw24yz/.cache/pip/wheels/98/e0/1f/91a11fdb0b701b93b6a6d05cb43a2f9ce2adff114186764e11
\par Successfully built temp
\par Installing collected packages: temp, namex, libclang, flatbuffers, wrapt, urllib3, typing-extensions, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, scipy, pygments, pyarrow, protobuf, opt-einsum, ml-dtypes, mdurl, MarkupSafe, markdown, joblib, idna, h5py, grpcio, google-pasta, gast, charset-normalizer, certifi, astunparse, absl-py, werkzeug, scikit-learn, requests, optree, markdown-it-py, tensorboard, rich, keras, tensorflow
\par Successfully installed Markup\cf1\highlight2 Sa\cf0\highlight0 fe-3.0.2 absl-py-2.1.0 ast\cf7 unparse-1.\cf0 6.3 certifi-2025.1.31 charset-normalizer-3.4.1 flatbuffers-25.2.10 gast-0.6\cf1\highlight2 .0\cf0\highlight0  google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 idna-3.10 joblib-1.4.2 keras-3.8.0 libcla\cf1\highlight2 ng\cf0\highlight0 -18.1.1 markdown-3.7 markd\cf7 own-it-py-\cf0 3.0.0 mdurl-0.1\cf1\highlight2 .2\cf0\highlight0  ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.0 proto\cf1\highlight2 bu\cf0\highlight0 f-5.29.3 pyarrow-19.0.1 pygments-2.19.1 requests-2.32.3 rich-13.9.4 sc\cf1\highlight2 ik\cf0\highlight0 it-learn-1.6.1 scipy-1.15.2 temp-202\cf1\highlight2 0.\cf0\highlight0 7.2 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-fil\cf1\highlight2 es\cf0\highlight0 ystem-0.37.1 termcolor-2.5.0 threadpoolctl-3.5.0 typing-extensions-4.12.2 urllib3-2.3.0 werkzeug-3.1.3 wrapt-1.17.2
\par (myspark) [acw24yz@node00\cf1\highlight2 1 \cf0\highlight0 [stanage] \cf1\highlight2 la\cf0\highlight0 b6]$ spark.conf.set("spark\cf1\highlight2 .s\cf0\highlight0 ql.execution.arrow.pyspark.e\cf1\highlight2 na\cf0\highlight0 bled", "true")
\par bash: syntax\cf1\highlight2  e\cf0\highlight0 rror near unexpected token `"spark.sql.exec\cf1\highlight2 ut\cf0\highlight0 ion.arrow\cf1\highlight2 .pys\cf0\highlight0 park.enabled",'
\par (myspark) [acw24yz@node001 [stanage] la\cf1\highlight2 b6\cf0\highlight0 ]$ pyspark
\par Python 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] on linux
\par \cf1\highlight2 Ty\cf0\highlight0 pe "help", "copyright", "credits" or "license" for more information.
\par Setting defa\cf1\highlight2 ul\cf0\highlight0 t log level to "WARN".
\par To adjust \cf1\highlight2 lo\cf0\highlight0 gging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(n\cf1\highlight2 ew\cf0\highlight0 Level).
\par 25/02/28 17:0\cf1\highlight2 6:\cf0\highlight0 04 WARN NativeCodeLoader: Unable to load native-hadoop \cf1\highlight2 li\cf0\highlight0 brary for your platfor\cf1\highlight2 m.\cf0\highlight0 .. using builtin-java classes where applicable
\par Welc\cf1\highlight2 om\cf0\highlight0 e to
\par       ___\cf1\highlight2 _ \cf0\highlight0              __
\par      / __/__  \cf1\highlight2 __\cf0\highlight0 _ _____/ /__
\par     _\\ \\/ _ \\/ _ `/ __/  '_/
\par    /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0
\par   \cf1\highlight2   \cf0\highlight0   /_/
\par 
\par Using Python version 3.11.7 (\cf1\highlight2 ma\cf0\highlight0 in, Dec 15 2023 18:12:31)
\par Sp\cf1\highlight2 ar\cf0\highlight0 k context Web UI availabl\cf1\highlight2 e \cf0\highlight0 at http://node001.pri.stanage.a\cf1\highlight2 lc\cf0\highlight0 es.network:4040
\par Spark context availa\cf1\highlight2 ble \cf0\highlight0 as 'sc' (master = local[*], app id = local-1740762366013).
\par SparkSession available as '\cf1\highlight2 sp\cf0\highlight0 ark'.
\par >>> spark.conf.set("spark.s\cf1\highlight2 ql\cf0\highlight0 .execution.arrow.pyspark.enabled"\cf1\highlight2 , \cf0\highlight0 "true")
\par >>> import numpy\cf1\highlight2  a\cf0\highlight0 s np
\par >>> # We load the dataset and the names of the features
\par >>> imp\cf1\highlight2 ort \cf0\highlight0 numpy as np
\par >>> rawdata = spark.re\cf1\highlight2 ad\cf0\highlight0 .csv('./Data/spambase.data')
\par rawdata.ca\cf1\highlight2 ch\cf0\highlight0 e()
\par ncolumns = len(rawdata.columns)\cf1\highlight2 
\par spa\cf0\highlight0 m_names = [spam_names.rstrip('\\n') for spam_names in open('./Data/spambase.data.names\cf1\highlight2 ')\cf0\highlight0 ]
\par number_names = np.sh\cf1\highlight2 ap\cf0\highlight0 e(spam_names)[0]
\par for i in range(number_names):
\par     local\cf1\highlight2  =\cf0\highlight0  spam_names[i]
\par    \cf1\highlight2  c\cf0\highlight0 olon_pos = local.find(':')
\par     spam_names[i] = local[:colon_pos]
\par 
\par # We rename the columns in the dataframe with names of the features in spamd.data.names
\par schemaNames = rawdata.schema.n\cf1\highlight2 am\cf0\highlight0 es
\par spam_names[ncolumns-1] = 'labels'
\par for i in range(ncolumns):
\par     rawdata = rawdata.withColumnRenamed(schemaNames[i], spam_names[i])
\par 
\par # We cast the type string to double
\par from pyspark.sql.types import StringType
\par from pyspark.sql.functions import col
\par 
\par StringColumns = [x.name for x in rawdata.schema.fields if x.dataType == StringType()]
\par for c in StringColumns:
\par     rawdata = rawdata.withColumn(c, col(c).cast("double"))
\par >>> rawdata.cache()
\par 25/02/28 17:06:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
\par DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, \cf1\highlight2 _c\cf0\highlight0 12: string, _c13: string, _c14: str\cf1\highlight2 in\cf0\highlight0 g, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: st\cf1\highlight2 ri\cf0\highlight0 ng, _c22: string, _c23: string, _c24: stri\cf1\highlight2 ng\cf0\highlight0 , _c25: string, _c26: string, _c2\cf1\highlight2 7:\cf0\highlight0  string, _c28: string, _c29: \cf1\highlight2 st\cf0\highlight0 ring, _c30: string, _c31: string, _\cf1\highlight2 c3\cf0\highlight0 2: string, _c33: string, _c34: string, _c\cf1\highlight2 35\cf0\highlight0 : s\cf1\highlight2 tr\cf0\highlight0 ing, _c36: string, _c37: string, _c38: string, _c39: string, _c40: string, _c41: string, _c\cf1\highlight2 42\cf0\highlight0 : string, _c43: string, _c44: string, \cf1\highlight2 _c\cf0\highlight0 45: string, _c46: string\cf1\highlight2 , \cf0\highlight0 _c47: string, _c48: string, _c49: stri\cf1\highlight2 ng\cf0\highlight0 , _c50: string,\cf1\highlight2  _\cf0\highlight0 c51: string, _c52: string, \cf1\highlight2 _c\cf0\highlight0 53: string, _c54: string, _c55: strin\cf1\highlight2 g,\cf0\highlight0  _c56: string, _c57: string]\cf1\highlight2 
\par >\cf0\highlight0 >> ncolumns = len(rawdata.columns)
\par >>> spam_names = [spam_names.rstrip('\\\cf1\highlight2 n'\cf0\highlight0 ) f\cf1\highlight2 or\cf0\highlight0  spam_names in open('./Data/spamba\cf1\highlight2 se\cf0\highlight0 .data.names')]
\par >>> number_names = np\cf1\highlight2 .s\cf0\highlight0 hape(spam_names)[0]
\par >>> for i in range(numb\cf1\highlight2 er\cf0\highlight0 _names):
\par ...     local = spam_names[i]\cf1\highlight2 
\par \cf0\highlight0 ...     colon_pos = local.find(':')
\par ...    \cf1\highlight2  s\cf0\highlight0 pam_names[i] = local[:colon_pos]
\par ...
\par ..\cf1\highlight2 . \cf0\highlight0 # W\cf1\highlight2 e \cf0\highlight0 rename the columns in the dataframe with names of the features in spamd.data.names
\par ... s\cf1\highlight2 ch\cf0\highlight0 emaNames = rawdata.schema.n\cf1\highlight2 am\cf0\highlight0 es
\par   File "<stdin>", line 7
\par     schemaNames = rawdata.schema\cf1\highlight2 .n\cf0\highlight0 ame\cf1\highlight2 s\cf0\highlight0 
\par   \cf1\highlight2   \cf0\highlight0 ^^^^^^^^^^^
\par SyntaxError: invalid syntax
\par >>> spam_names[ncolumn\cf1\highlight2 s-\cf0\highlight0 1] \cf1\highlight2 = \cf0\highlight0 'labels'
\par >>> for i in range(ncolumns):
\par ...     r\cf1\highlight2 aw\cf0\highlight0 data = rawdata.withColumnRenamed(schemaNames[i], spam_names[i])
\par ...
\par Traceback (most recent cal\cf1\highlight2 l \cf0\highlight0 las\cf1\highlight2 t)\cf0\highlight0 :
\par   File "<stdin>", line 2, in <module>
\par NameError: name 'schemaNames' \cf1\highlight2 is\cf0\highlight0  not defined
\par >>> # We cast the type string to double
\par >>> from pyspark\cf1\highlight2 .s\cf0\highlight0 ql.types import StringType
\par >>> from pyspark.sql.fun\cf1\highlight2 ct\cf0\highlight0 ions import col
\par >>>
\par >>> StringColumns = [x.name for x in rawdata.schema.fields if x.dataType == StringType()]
\par >>> for c\cf1\highlight2  i\cf0\highlight0 n S\cf1\highlight2 tr\cf0\highlight0 ingColumns:
\par ...     rawd\cf1\highlight2 at\cf0\highlight0 a = rawdata.withColumn(c, col(c).ca\cf1\highlight2 st\cf0\highlight0 ("double"))
\par ...
\par >>>
\par >>> train\cf1\highlight2 in\cf0\highlight0 gData, testData = rawdata.randomSplit(\cf1\highlight2 [0\cf0\highlight0 .7, 0.3], 42)
\par >>>
\par >>> from pyspark.ml.fe\cf1\highlight2 at\cf0\highlight0 ure import VectorAssembler
\par >>> vecAssembler =\cf1\highlight2  V\cf0\highlight0 ectorAssembler(inputCols \cf1\highlight2 = \cf0\highlight0 spam_names[0:ncolumns-1], outputCol = 'features\cf1\highlight2 ')\cf0\highlight0 
\par >>>
\par >>> from pyspark.ml.classification import MultilayerPerceptr\cf1\highlight2 on\cf0\highlight0 Classifier
\par >>> # The first element HAS to be \cf1\highlight2 eq\cf0\highlight0 ual to the number of input features
\par >>> layers = [len(trainingData.columns)\cf1\highlight2 -1\cf0\highlight0 , 20, 5, 2]
\par >>> mpc = MultilayerPerceptro\cf1\highlight2 nC\cf0\highlight0 lassifier(labelCol="labels", featuresCol="features", maxIter=100, la\cf1\highlight2 ye\cf0\highlight0 rs=layers, seed=1500)
\par >>>
\par >>> # Cre\cf1\highlight2 at\cf0\highlight0 e the pipeline
\par >>> from pyspark.ml import Pipeline
\par >>> stages = [vecAssembler, mpc]
\par >>> pipeline = Pipelin\cf1\highlight2 e(\cf0\highlight0 stages=stages)
\par pipelineModel\cf1\highlight2  =\cf0\highlight0  pipeline.fit(trainingData)
\par \cf1\highlight2 >>\cf0\highlight0 > pipelineModel = pipeline.fit(trainingData)
\par # We now make predictions
\par predictions = pipelineModel.transform(tes\cf1\highlight2 tD\cf0\highlight0 ata)
\par from pyspark.ml.evaluation impor\cf1\highlight2 t \cf0\highlight0 MulticlassClassificationEvaluator
\par eva\cf1\highlight2 lu\cf0\highlight0 ator = MulticlassClassificationEvaluator\\
\par       (labelCol="labels", predictionCol="prediction", metricName="accura\cf1\highlight2 cy\cf0\highlight0 ")
\par accuracy = evaluator.evaluate(p\cf1\highlight2 re\cf0\highlight0 dictions)
\par print("Accuracy = %g " %\cf1\highlight2  a\cf0\highlight0 ccuracy)Traceback (most recent call last):
\par   File "<stdin>", line 1, in <module>
\par   File "/users/acw24yz/.conda/envs/m\cf1\highlight2 ys\cf0\highlight0 park/lib/python3.11/site-packages/pyspark/ml/base.py", line 205, in fit
\par     retur\cf1\highlight2 n \cf0\highlight0 self._fit(dataset)
\par            ^^^^^^^^^^^^^^^^^^
\par   File "\cf1\highlight2 /u\cf0\highlight0 sers/acw24yz/.conda/envs/myspark/lib/python3.11/site-packages/pyspark/ml/pipeline.py", line 132, in _fit
\par     dataset = stage.transform(dataset)
\par             \cf1\highlight2   \cf0\highlight0 ^^^^^^^^^^^^^^^^^^^^^^^^
\par   File "/users/acw24yz/.conda/envs/myspark/lib/python3.11/site-packages/pyspark/ml/base.py", line 262, \cf1\highlight2 in\cf0\highlight0  transform
\par     return self._\cf1\highlight2 tr\cf0\highlight0 ansform(dataset)
\par            ^^^^^^^^^^^^^^^^^^^^^^^^
\par   File "/users/acw24yz/.conda/envs/myspark/lib/python3.11/site-packages/pyspark/ml/wrapper.py", line 398, in _transform
\par     return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sparkSession)
\par                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\par   File "/users/acw24yz/.conda/envs/myspark/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", lin\cf1\highlight2 e \cf0\highlight0 132\cf1\highlight2 2,\cf0\highlight0  in __call__
\par   File "/users/\cf1\highlight2 ac\cf0\highlight0 w24yz/.conda/envs/myspark/lib/python3.11/site-packa\cf1\highlight2 ge\cf0\highlight0 s/pyspark/errors/exceptions/captur\cf1\highlight2 ed\cf0\highlight0 .py", line 185, in deco
\par     raise co\cf1\highlight2 nv\cf0\highlight0 erted from None
\par pyspark.errors.exceptions.cap\cf1\highlight2 tu\cf0\highlight0 red.IllegalArgumentException: word_freq_make:         continuous. does \cf1\highlight2 no\cf0\highlight0 t exist. Available: _c0, _c1, _c2, _c3, _c4, _c5, \cf1\highlight2 _c\cf0\highlight0 6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c\cf1\highlight2 20\cf0\highlight0 , _c21, _c22, _c23, _c24, _c25, _c26, _c27, _c\cf1\highlight2 28\cf0\highlight0 , _c29, _c30, _c31, _c32, _c33, _c\cf1\highlight2 34\cf0\highlight0 , _c35, _c36, _c37, _c38, _c39, _c40,\cf1\highlight2  _\cf0\highlight0 c41, _c42, _c43, _c44, _c45, _c46, _c47, _c4\cf1\highlight2 8,\cf0\highlight0  _c49, _c50, _c51, _c52, _c53, _c54, _\cf1\highlight2 c5\cf0\highlight0 5, _c56, _c57
\par >>>
\par >>> # We now m\cf1\highlight2 ak\cf0\highlight0 e predictions
\par >>> predictions = pipe\cf1\highlight2 li\cf0\highlight0 neModel.transform(testData)
\par Traceback (m\cf1\highlight2 os\cf0\highlight0 t r\cf1\highlight2 ec\cf0\highlight0 ent\cf1\highlight2  c\cf0\highlight0 all last):
\par   File "<stdin\cf7 >", line 1\cf0 , in <module>
\par \cf1\highlight2 Na\cf0\highlight0 meError: name 'pipelineModel' is not defined
\par >>> from pyspark.ml\cf1\highlight2 .e\cf0\highlight0 valuation import MulticlassClassificationEvaluator
\par >>> evaluator = Mu\cf1\highlight2 lt\cf0\highlight0 iclassClassificationEvaluator\\
\par ... \cf1\highlight2   \cf0\highlight0     (labelCol="labels", predictionCol="prediction", metricName="accuracy")
\par >>> accuracy = e\cf1\highlight2 va\cf0\highlight0 luator.evaluate(predictions)
\par Traceback (most recent call last):
\par   File "<stdin>", line 1, in <module>
\par NameError: name 'predictions' is not \cf1\highlight2 de\cf0\highlight0 fined
\par >>>\cf1\highlight2  p\cf0\highlight0 rint("Accuracy = %g " % ac\cf1\highlight2 cu\cf0\highlight0 racy)
\par Traceback (most recen\cf1\highlight2 t \cf0\highlight0 call last):
\par   File "<stdin>\cf1\highlight2 ",\cf0\highlight0  line 1, in <module>
\par NameError: name 'accu\cf1\highlight2 ra\cf0\highlight0 cy' is no\cf1\highlight2 t de\cf0\highlight0 fined
\par >>>
\par >>>
\par (myspark) [acw24yz@node001 [stanage] la\cf1\highlight2 b6\cf0\highlight0 ]$ pyspark
\par Python 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] on linux
\par \cf1\highlight2 Ty\cf0\highlight0 pe "help", "copyright", "credits" or "license" for more information.
\par Setting defa\cf1\highlight2 ul\cf0\highlight0 t log level to "WARN".
\par To adjust \cf1\highlight2 lo\cf0\highlight0 gging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(n\cf1\highlight2 ew\cf0\highlight0 Level).
\par 25/02/28 17:0\cf1\highlight2 8:\cf0\highlight0 03 WARN NativeCodeLoader: Unable to load native-hado\cf1\highlight2 op l\cf0\highlight0 ibrary for your pla\cf1\highlight2 tf\cf0\highlight0 orm... using builtin-java classes where applicable
\par Welcome to
\par       ____              __
\par      / __/__  ___ _____/ /__
\par     _\\ \\/ _ \\/ _ `/ __/  '_/
\par    /__ / .__/\\_,_/_/ /_/\\_\\   versio\cf1\highlight2 n \cf0\highlight0 3.5.0
\par       /_/
\par 
\par Using Python version 3.11.7 (main, Dec 15 2023 18:12:31)
\par Spark context Web UI available at http://node001.pri.stanage.alces.network:4040
\par Spark context available as 'sc' (master = local[*], app id = local-1740762485084).
\par SparkSession available as 'spark'.
\par >>> spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
\par >>> import numpy as np
\par >>> rawdata = spark.read.csv('./Data/spambase.data')
\par 
\par >>> rawdata.cache()
\par 25/02/28 17:08:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
\par DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, \cf1\highlight2 _c\cf0\highlight0 12: string, _c13: string, _c14: str\cf1\highlight2 in\cf0\highlight0 g, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: st\cf1\highlight2 ri\cf0\highlight0 ng, _c22: string, _c23: string, _c24: stri\cf1\highlight2 ng\cf0\highlight0 , _\cf1\highlight2 c2\cf0\highlight0 5: string, _c26: string, _c27: st\cf1\highlight2 ri\cf0\highlight0 ng, _c28: string, _c29: strin\cf1\highlight2 g,\cf0\highlight0  _c30: string, _c31: string, _c32: \cf1\highlight2 st\cf0\highlight0 ring, _c33: string, _c34: string, _c35: s\cf1\highlight2 tr\cf0\highlight0 ing\cf1\highlight2 , \cf0\highlight0 _c36: string, _c37: string, _c38: stri\cf1\highlight2 ng\cf0\highlight0 , _c39: string, _c40: string, _c41: s\cf1\highlight2 tr\cf0\highlight0 ing\cf1\highlight2 , \cf0\highlight0 _c42: string, _c43: string, _\cf1\highlight2 c4\cf0\highlight0 4: string, _c45: string, _c46: string, _c47: string, _c48: string, _c49: s\cf1\highlight2 tr\cf0\highlight0 ing\cf1\highlight2 , \cf0\highlight0 _c50: string, _c51: string, _c52: string, _c\cf1\highlight2 53\cf0\highlight0 : string, _c54: string, _c55: string, _c5\cf1\highlight2 6:\cf0\highlight0  st\cf1\highlight2 ri\cf0\highlight0 ng, _c57: string]
\par >>> ncolumns = len(rawdata.columns)
\par >>> spam_names = [spam_names.rstr\cf1\highlight2 ip\cf0\highlight0 ('\\\cf1\highlight2 n'\cf0\highlight0 ) for spam_names in open('.\cf1\highlight2 /D\cf0\highlight0 ata/spambase.data.names')]
\par >>> number_names = np.shape(spam_n\cf1\highlight2 am\cf0\highlight0 es)\cf1\highlight2 [0\cf0\highlight0 ]
\par >>>
\par >>> for i in range(number_names):
\par ...     local = spam_\cf1\highlight2 na\cf0\highlight0 mes\cf1\highlight2 [i\cf0\highlight0 ]
\par ...     colon_pos = local.find(':')
\par ...     sp\cf1\highlight2 am\cf0\highlight0 _names[i] = local[:colon_pos]
\par ...
\par >>> schemaNames = rawdata.schema.names
\par >>> spam_names[ncolu\cf1\highlight2 mn\cf0\highlight0 s-1\cf1\highlight2 ] \cf0\highlight0 = 'labels'
\par >>>
\par >>> for i in range(ncolumns):
\par ..\cf1\highlight2 . \cf0\highlight0     rawdata = rawdata.withColumnRenamed(schemaNames[i], spam_names[i])
\par ...
\par >>> from pyspark.sq\cf1\highlight2 l.\cf0\highlight0 typ\cf1\highlight2 es\cf0\highlight0  import StringType
\par >>> from pyspark.sql.functions import col
\par >>>
\par >>> \cf1\highlight2 St\cf0\highlight0 ringColumns = [x.name for x in rawdata.schema.fields\cf1\highlight2  i\cf0\highlight0 f x.dataType == StringType()]
\par >>>
\par >>> for c in StringColumns:
\par ...     rawdata = rawdata.withColumn(c, col(c).cast("doub\cf1\highlight2 le\cf0\highlight0 "))\cf1\highlight2 
\par \cf0\highlight0 ...
\par >>> trainingData, testData = r\cf1\highlight2 aw\cf0\highlight0 data.randomSplit([0.7, 0.3], 42)\cf1\highlight2 
\par \cf0\highlight0 >>>
\par >>> from pyspark.ml.feature impor\cf1\highlight2 t \cf0\highlight0 VectorAssembler
\par >>> vecAssembler = VectorAsse\cf1\highlight2 mb\cf0\highlight0 ler(inputCols = spam_names[0:ncolumns-1], outputCol = 'features')
\par >>>
\par >>> from pyspark.ml.feature import\cf1\highlight2  V\cf0\highlight0 ectorAssembler
\par >>> vecAssembler = VectorAssembler(inputCols = spam_names[0:ncolumns-1], outputCol = 'feature\cf1\highlight2 s'\cf0\highlight0 )
\par \cf1\highlight2 >>\cf0\highlight0 >
\par >>> from pyspark.ml.classification import Multil\cf1\highlight2 ay\cf0\highlight0 erPerceptronClassifier
\par >>> layers = [len(trainingData.columns)-1, 20, \cf1\highlight2 5,\cf0\highlight0  2]
\par >>> mpc = MultilayerPerceptronClassifier(labe\cf1\highlight2 lC\cf0\highlight0 ol="labels", featuresCol="features", maxIter=100, layers=layers, seed=1500)
\par >>>\cf1\highlight2 
\par \cf0\highlight0 >>>\cf1\highlight2  f\cf0\highlight0 rom pyspark.ml import Pipeline
\par >>> stages = [\cf1\highlight2 ve\cf0\highlight0 cAssembler, mpc]
\par >>> pipeline = Pipel\cf1\highlight2 in\cf0\highlight0 e(stages=stages)
\par >\cf1\highlight2 >>\cf0\highlight0  pi\cf1\highlight2 pe\cf0\highlight0 lin\cf1\highlight2 eM\cf0\highlight0 ode\cf1\highlight2 l =\cf0\highlight0  pipeline.fit(trainingData)
\par 25/02/28 17:11:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
\par 25/02/28 17:11:09 WARN InstanceBuilder: Failed to load implementation from:dev.lud\cf1\highlight2 ov\cf0\highlight0 ic.netlib.blas.VectorBLAS
\par >>>
\par >>> predictions = pipelineModel.transform(testData)
\par >>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator
\par >>> evaluator = MulticlassClassificationEvaluator\\
\par ...       (labelCol="labels", predictionCol="prediction", metricName="accuracy")
\par >>>
\par >>> accuracy = evaluator.evaluate(predictions)
\par >>> print("Accuracy = %g " % accuracy)
\par Accuracy = 0.842346
\par >>>
\par >>>
\par >>>
\par \pard\f1 
\par }
 